{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMVnWGXTNL3NIp4+woXSEU4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaroorhmodi/transformer-from-scratch/blob/main/transformer_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Building a Transformer from Scratch\n",
        "\n",
        "Using the famous paper [***Attention Is All You Need***](https://arxiv.org/pdf/1706.03762.pdf), we will build a transformer model from scratch and train it on the ~**tiny_shakespeare** dataset featured in [Andrej Karpathy's article](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)~. The new dataset is yet to be determined. Thinking about training it on the [Stanford Question Answering Dataset (SQuAD)](https://paperswithcode.com/dataset/squad). I am not expecting performance nearly as good as transfer learning with T5 but I would like to see something resembling good QA reasoning.\n",
        "\n",
        "Following [*The Annotated Transformer*](http://nlp.seas.harvard.edu/annotated-transformer/#training-data-and-batching) along with this [Towards Data Science post](https://towardsdatascience.com/all-you-need-to-know-about-attention-and-transformers-in-depth-understanding-part-1-552f0b41d021#8607) loosely to clarify any confusions when reading the paper.\n",
        "\n",
        "I will **NOT** be training the model on the same amount of data as the original paper does. It's about 3.5 days of training on 8 GPUs, something I cannot do at the moment. Perhaps I will attempt bigger models in the future! :)"
      ],
      "metadata": {
        "id": "KZktnWMrup34"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Preliminary Work"
      ],
      "metadata": {
        "id": "aX4xdt9_yk-b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torchdata==0.3.0 torchtext==0.12 spacy==3.2 altair GPUtil"
      ],
      "metadata": {
        "id": "zAC5uWTxuQ2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4LW_EzhFh-H"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import math\n",
        "import copy\n",
        "import time\n",
        "import pandas aspd\n",
        "import altair as alt\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "\n",
        "RUN_EXAMPLES = True\n",
        "\n",
        "#I will only ever run this on GPU\n",
        "#but I want to make this device agnostic.\n",
        "\n",
        "#FUTURE GOALS: Try to do multi-gpu training in colab in the\n",
        "#future with a newer, more ambitious paper to duplicate\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###PAPER COMMENTS:\n",
        "**Section 1** of the paper basically breaks down part of the motivation behind including attention mechanisms in NLP models. I am familiar with how LSTM and other RNN architectures work and it seems like the impetus (at the time) was to capture inter-word dependencies in a way that is not inherently sequential. This allows for training efficiency to improve by leveraging parallel computation (*which I am funnily enough eschewing in this exercise I am undertaking*). This parallelization effectively increases the effectiveness of the model by allowing for training on larger sets of data.\n",
        "\n",
        "**Section 2** of the paper breaks down how attention compares to existing approaches employing convolution and sequence-aligned RNNS ([which I think refers to Seq2Seq models](https://towardsdatascience.com/how-to-implement-seq2seq-lstm-model-in-keras-shortcutnlp-6f355f3e5639)).\n",
        "\n",
        "Attention has an advantage in learning relations between distant word positions (constant number of operations) but it loses some resolution. The Transformer employs **Multi-Head-Attention** to counteract this. This is described later in the paper."
      ],
      "metadata": {
        "id": "rNayhR5Pypt2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model Architecture"
      ],
      "metadata": {
        "id": "Pfz-7XCcMleJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the paper, model architecture is covered in **Section 3**.\n",
        "\n",
        "The transformer model is similar to prior popular models like *LSTM* in that it has an **encoder-decoder** structure. Encoders map input sequences (symbol representation) to continuous representations which the decoder then maps to symbols. this happens one element at a time and is auto-regressive; which is to say that all previously generated symbols become inputs as the next output symbol is generated.\n",
        "\n",
        "For $\\mathbf{x} = (x_1,…, x_n)\\in \\mathbf{W}$, $\\mathbf{y} = (y_1,…, y_m)\\in \\mathbf{W'}$, $\\mathbf{z} = (z_1,…, z_n)\\in \\mathbb{R}^n$, the mapping looks something like this:\n",
        "$$\\mathbf{x}→\\mathbf{z}→\\mathbf{y}$$\n",
        "Above we have $\\mathbf{W}$ and $\\mathbf{W'}$ to represent word vocabularies which may or may not be in the same language (different languages might be used in a machine translation task like the original transformer paper used). Although even in the case of the machine translation use case, vocabularies are merged for both input and output since the weight vector of the embedding is shared for input and output embeddings."
      ],
      "metadata": {
        "id": "4KRhpO41Mqqd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Encoder-Decoder Architecture](https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1-768x1082.png)"
      ],
      "metadata": {
        "id": "-d_KySRMIPdf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderDecoder(nn.Module):\n",
        "  \"\"\"\n",
        "  This is a skeleton for the encoder-decoder structuer mentioned above.\n",
        "\n",
        "  I am following the code organization in the Annotated Transformer article mentioned above.\n",
        "  \"\"\"\n",
        "  def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
        "    super(EncoderDecoder, self).__init__()\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.src_embed = src_embed\n",
        "    self.tgt_embed = tgt_embed\n",
        "    self.generator = generator\n",
        "\n",
        "  def encode(self, src, src_mask):\n",
        "    \"\"\"\n",
        "    Forward pass of the encoder. Embed the src text\n",
        "    and mask it. Then you run a fwd pass of the encoder on it.\n",
        "\n",
        "    This is the shell of the first half of the transformer's\n",
        "    forward pass. We will code the specfics of this when we\n",
        "    design the encoder.\n",
        "    \"\"\"\n",
        "    return self.encoder(self.src_embed(src), src_mask)\n",
        "\n",
        "  def decode(self, memory, src_mask, tgt, tgt_mask):\n",
        "    \"\"\"\n",
        "    This is the shell of the second half of the forward pass of\n",
        "    the transformer. We will also specify this further ahead.\n",
        "\n",
        "    The decoder has a memory component because it also takes in\n",
        "    the already generated words in its output.\n",
        "    \"\"\"\n",
        "    return self.decoder(self.tgt_embed(tgt), memory, tgt_mask)"
      ],
      "metadata": {
        "id": "yOovacTuOOaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***The Annotated Transformer*** also defines the generator portion of the transformer here. This is given in the ***Attention is All You Need*** paper by the linear portion following the decoder along with the softmax step following it. This linear layer is not the same as the linear layers that are in both the encoder and decoder."
      ],
      "metadata": {
        "id": "Skj7W9ib8MrZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "  \"\"\"\n",
        "  As described above, the Generator follows the Decoder.\n",
        "  This portion seems relatively simple.\n",
        "  It's simply a fully connected Linear step followed by softmax.\n",
        "  \"\"\"\n",
        "  def __init__(self, d_model, vocab):\n",
        "    super(Generator, self).__init__()\n",
        "    self.proj = nn.Linear(d_model, vocab)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return nn.log_softmax(self.proj(x), dim=1)"
      ],
      "metadata": {
        "id": "1qwbXpvz8MYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Section 3.1**: The paper describes the nature of the Encoder and Decoder here.\n",
        "\n",
        "###Encoder\n",
        "    \n",
        "The encoder has $N$ stacked layers that are all the same. Each layer has a Multi-Head Attention sublayer that feeds into a position-wise fully connected feed-forward network.The output of each sublayer is given by $\\text{LayerNorm}(x + \\text{Sublayer}(x))$ where $\\text{Sublayer}(x)$ is the forward pass function of the sublayer in question.\n",
        "\n",
        "`LayerNorm` is layer normalization. **The Annotated Transformer** implements it manually so I will do the same to avoid issues further ahead but I believe what is being done here is very similar to the implementation in [`torch.nn.LayerNorm`](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html).\n",
        "\n",
        "There is also the question of a residual connection between the sublayers alongside the norm. In the paper it is mentioned that a sublayer dropout is implemented before the normalization with $P_{\\text{drop}}=0.1$ in **Section 5.4**.\n",
        "\n",
        "In the paper the encoder has $N=6$ stacked layers."
      ],
      "metadata": {
        "id": "8cQ37aw4HSor"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "  def __init__(self, features, eps=1e-6):\n",
        "    super(LayerNorm, self).__init__()\n",
        "    self.a_2 = nn.Parameter(torch.ones(features))\n",
        "    self.b_2 = nn.Parameter(torch.zeros(features))\n",
        "    self.eps = eps\n",
        "\n",
        "  def forward(self, x):\n",
        "    mean = x.mean(-1, keepdim=True)\n",
        "    std = x.std(-1, keepdim=True)\n",
        "    return self.a_2*(x-mean)/(std+self.eps) +self.b_2"
      ],
      "metadata": {
        "id": "NSlYM-YHN2MQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SublayerConnection(nn.Module):\n",
        "  \"\"\"\n",
        "  Residual Connection with dropout followed by LayerNorm.\n",
        "\n",
        "  This accomplishes the aforementioned residual connection\n",
        "  with normalization used between the sublayers of the Encoder.\n",
        "\n",
        "  dropout has default prob=0.1 like in the paper.\n",
        "  \"\"\"\n",
        "  def __init__(self, size, dropout=0.1):\n",
        "    super(SublayerConnection, self).__init__()\n",
        "    self.norm = LayerNorm(size)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x, sublayer):\n",
        "    return x+self.dropout(sublayer(self.norm(x)))\n"
      ],
      "metadata": {
        "id": "__2cPxY7Q_bf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I think I see why this is self-implemented. This will do LayerNorm for multiple stacked layers as a single layer in an `EncoderLayer` implementation."
      ],
      "metadata": {
        "id": "do---aJfPk3e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clones(module, N):\n",
        "  #produces N-stack of identical layers\n",
        "  return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self, layer, N):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.layers = clones(layer, N)\n",
        "    self.norm = LayerNorm(layer.size)\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "    \"Forward pass of the encoder. Include mask.\"\n",
        "    for layer in self.layers:\n",
        "      x = layer(x, mask)\n",
        "    return self.norm(x)"
      ],
      "metadata": {
        "id": "c9Boxl8KyoYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "  \"\"\"\n",
        "  This is the fundamental architecture of a single Encoder Layer.\n",
        "\n",
        "  The encoder will be made up of N stacked copies of this (N=6).\n",
        "\n",
        "  We have yet to implement the key feature: Multi-Head Attention.\n",
        "\n",
        "  This will be implemented and explained below.\n",
        "  \"\"\"\n",
        "  def __init__(self, size, self_attn, feed_forward, dropout=0.1):\n",
        "    super(EncoderLayer, self).__init__()\n",
        "    self.self_attn = self_attn #multi-head attention layer\n",
        "    self.feed_forward = feed_forward\n",
        "    # N=2 sublayer connections here are not to be thought of\n",
        "    # as \"stacked\" but rather they are just the sublayer connection\n",
        "    # following the MHA layer and then the feedforward layer.\n",
        "    self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
        "    self.size = size\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "    x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
        "    return self.sublayer[1](x, self.feed_forward)\n"
      ],
      "metadata": {
        "id": "8WWmtpG2Tghz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Decoder\n",
        "    \n",
        "The decoder is similar to the encoder in that it employs $N=6$ stacked layers that are exactly alike. It also shares a similar sublayer connection to the Encoder but it has *two* MHA sublayers.\n",
        "\n",
        "The first performs masked MHA over the output embedding (the mask is there to prevent signal from later words in the output from reaching prediction on earlier words).\n",
        "\n",
        "The second sublayer performs MHA over the output from the encoder and maintains the sublayer scheme from before.\n",
        "\n",
        "This all goes into the third sublayer (feed-forward) in a similar fashion to the encoder layer."
      ],
      "metadata": {
        "id": "i7eRs5HQMmq8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "\n",
        "  def __init__(self, layer, N):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.layers = clones(layer, N)\n",
        "    self.norm = LayerNorm(layer.size)\n",
        "\n",
        "  def forward(self, x, memory, src_mask, tgt_mask):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x, memory, src_mask, tgt_mask)\n",
        "      return self.norm(x)"
      ],
      "metadata": {
        "id": "Gkq0uBseMble"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will make the `DecoderLayer` class using the `SublayerConnection` class we made before. MHA is yet to be implemented, it will be accounted for in the `DecoderLayer` class."
      ],
      "metadata": {
        "id": "A9aF4pNVpRtr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "\n",
        "  def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "    self.size = size\n",
        "    self.self_attn = self_attn\n",
        "    self.src_attn = src_attn\n",
        "    self.feed_forward = feed_forward\n",
        "    self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
        "\n",
        "  def forward(self, x, memory, src_mask, tgt_mask):\n",
        "    m = memory\n",
        "    x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
        "    x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n",
        "    return self.sublayer[2](x, self.feed_forward)"
      ],
      "metadata": {
        "id": "sVlN6-qOpHcQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to include the mask to mask out subsequent words in the output. This way we avoid the signal from later words (or positions) in the output from reaching earlier ones."
      ],
      "metadata": {
        "id": "XQUk3jSEshHR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def subsequent_mask(size):\n",
        "  attn_shape = (1,shape,shape)\n",
        "  subsequent_mask = torch.triu(torch.ones(attn_shape), diagonal=1).type(torch.uint8)\n",
        "  return subsequent_mask == 0"
      ],
      "metadata": {
        "id": "aODuGPzDtLtN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Attention\n",
        "\n",
        "In the paper there are two parts to *Multi-Headed Attention*. There is **Scaled Dot-Product Attention** which feeds into **Multi-Headed Attention**."
      ],
      "metadata": {
        "id": "gFdP7Xovx58Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://machinelearningmastery.com/wp-content/uploads/2022/03/dotproduct_1.png)"
      ],
      "metadata": {
        "id": "5VrCjoXtz8p2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The reason we scale dot product attention by $\\frac{1}{\\sqrt{d_k}}$ is because as $d_k$ (the dimension of the query and key vectors) goes up, the dot products grow large in magnitude. This makes `softmax` have very small gradients which adversely affects model performance. The scaling of dot products counteracts this precise issue.\n",
        "\n",
        "Imagine the components of $Q$, $K$ are random variables with mean=0 and variance=1. This means their dot-product has mean=0 and variance=$d_k$=$d_q$."
      ],
      "metadata": {
        "id": "e2WyEGVU8kqV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#First we implement Scaled Dot Product Attention\n",
        "def attention(Q:torch.Tensor, K:torch.Tensor, V:torch.Tensor, mask=None, dropout=None):\n",
        "  d_k = Q.size(-1) #d_k is the dimension of queries and keys\n",
        "  #The reason we divide by d_k is given above.\n",
        "  scores = torch.matmul(Q, K.transpose(-2,-1)) / math.sqrt(d_k)\n",
        "  if mask:\n",
        "    #we fill zeroes with something like -inf\n",
        "    #this is to bring them close to 0 in softmax\n",
        "    scores = scores.masked_fill(mask == 0, -1e9)\n",
        "  if dropout:\n",
        "    p_attn = dropout(p_attn)\n",
        "  return torch.matmul(p_attn, V), p_attn\n"
      ],
      "metadata": {
        "id": "-EiIWFi0z8OC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Section 3.2.2**:\n",
        "\n",
        "We will implement Multi-Headed Attention here. This is the crux of the Transformer model.\n",
        "\n",
        "I will comment anything I find helpful in explaining the structure in the code itself. This is because even though the general intuition behind attention makes sense, the whole point of this exercise is to understand the specifics of its implementation in the Transformer model."
      ],
      "metadata": {
        "id": "Wq8CQyRG6Ys8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The \"*multi-head*\" part of MHA is accomplished by a series of $h$ learned projections where $h$ is the number of heads. We have $h=8$ parallel attention layers. Each of these project $Q$, $K$, $V$ into $d_k$, $d_k$, and $d_v$ dimensions via learned parameter matrices described below.\n",
        "\n",
        "$$i \\in \\{1,…,h\\}$$\n",
        "$$W_i^Q, W_i^K \\in \\mathbb{R}^{d_{\\text{model}}×d_k}$$\n",
        "$$W_i^V \\in \\mathbb{R}^{d_{\\text{model}}×d_v}$$\n",
        "$$W^O \\in \\mathbb{R}^{hd_{v}×d_{\\text{model}}}$$\n",
        "\n",
        "Each of the projections for a head are given:\n",
        "$$QW_i^Q, KW_i^K, VW_i^V$$\n",
        "\n",
        "And the attention function of each head is modeled:\n",
        "$$\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$$\n",
        "\n",
        "The way we bring these heads' output together is by concatenating and mapping back into the model space $\\mathbb{R}^{d_{\\text{model}}}$ with the parameter matrix $W^O$.\n",
        "\n",
        "So the Multi-Head Attention function is given:\n",
        "$$\\text{MultiHead}(Q,K,V) = \\text{Concat}(\\text{head}_1,…,\\text{head}_h)W^O$$"
      ],
      "metadata": {
        "id": "f2Q5LxrM-j5e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Implementing Multi-Head Attention\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, h, d_model, dropout=0.1):\n",
        "    super(MultiHeadAttention, self).__init()\n",
        "    #make sure model dimension is divisible into h heads\n",
        "    assert d_model % h == 0\n",
        "    #Remember the note above, in this paper:\n",
        "    #𝑑_𝑞=𝑑_𝑘=𝑑_𝑣=𝑑_model/ℎ\n",
        "    self.d_k = d_model//h #this will act as d_q ,d_v for us\n",
        "    self.h = h\n",
        "    #TODO: what are the cloned linears for? FOR PROJECTION\n",
        "    #TODO: Why are there FOUR and not THREE? LAST ONE IS FOR FINAL LINEAR\n",
        "    self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
        "    self.attn = None\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "  def forward(self, query, key, value, mask=None):\n",
        "    if mask is not None:\n",
        "      #apply the same mask to all the heads\n",
        "      mask = mask.unsqueeze(1)\n",
        "    nbatches = query.size(0)\n",
        "\n",
        "    #now we do the linear projections from d_model => d_k\n",
        "    #the projections for Q, K, V are all the same dimension\n",
        "\n",
        "    #TODO: map out dimensions of each Q, K, V transform here\n",
        "\n",
        "    query, key, value = [\n",
        "        #apply linear projections\n",
        "        lin(x).view(nbatches, -1, self.h, self.d_k).transpose(1,2)\n",
        "        for lin, x in zip(self.linears, (query, key, value))\n",
        "    ]\n",
        "\n",
        "    #apply attention on projections\n",
        "    x, self.attn = attention(\n",
        "        query, key, value, mask=mask, dropout=self.dropout\n",
        "    )\n",
        "\n",
        "    #Concat with a view and apply a linear layer\n",
        "    x = (\n",
        "        x.transpose(1,2)\n",
        "        .contiguous()\n",
        "        .view(nbatches, -1, self.h*self.d_k)\n",
        "    )\n",
        "    del query\n",
        "    del key\n",
        "    del value\n",
        "    return self.linears[-1](x)"
      ],
      "metadata": {
        "id": "4uwG9jWrv2-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that in the **Attention is All You Need** paper, $h=8$ heads (parallel layers) are used. And in this case, $d_q=d_k=d_v=d_{\\text{model}}/h = 64$.\n",
        "\n",
        "**Section 3.2.3**:\n",
        "\n",
        "#### Encoder Self-Attention:\n",
        "In the encoder self-attention layer the queries, keys, and values all come from the prior layer's output. Each position can attend to every position in the prior layer.\n",
        "\n",
        "We see this in the code for the `EncoderLayer` class when we use\n",
        "\n",
        "    x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
        "\n",
        "in the forward pass. Here, the mask will be `None` since no masking is needed.\n",
        "\n",
        "#### Decoder Self-Attention:\n",
        "Decoder self attention is similar to encoder attention except we employ the mask seen in `subsequent_mask` to mask out subsequent positions to prevent backward information flow in the output. We see how this essentially turns the illegal connections to `-inf` in the `attention` function with the line `scores = scores.masked_fill(mask == 0, -1e9)`. We see this in the `DecoderLayer` code forward pass in this line:\n",
        "\n",
        "    x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
        "\n",
        "#### Encoder-Decoder Attention:\n",
        "In this case the queries come from the previous decoder layer while the keys and values come from the `memory` generated by the `Encoder` output. This is identical to *Seq2Seq* models' encoder-decoder attention. This is seen in the line immediately following the one where decoder self-attention is implemented in `DecoderLayer`:\n",
        "\n",
        "    x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))"
      ],
      "metadata": {
        "id": "-cgQJo20MfxZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Position-Wise FeedForward\n",
        "**Section 3.3** outlines how the Position-wise Feed-Forward networks we use in the Transformer are defined.\n",
        "\n",
        "The basic function defined is a composition of two linear layers with a ReLU activation between them:\n",
        "\n",
        "$$\\text{FFN}(x)=\\text{max}(0,xW_1+b_1)W_2+b_2$$\n",
        "\n",
        "In the paper $d_{\\text{model}}/8=64$ so $d_{\\text{model}} = 512$. We are also given the inner-layer dimension to be: $d_{ff}=2048=d_{\\text{model}}*4$"
      ],
      "metadata": {
        "id": "btXMh-AeaE6p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionwiseFeedForward(nn.Module):\n",
        "  def__init__(self, d_model, d_ff, dropout=0.1):\n",
        "    super(PositionwiseFeedForward, self).__init__()\n",
        "    self.w_1 = nn.Linear(d_model, d_ff)\n",
        "    self.w_2 = nn.Linear(d_ff, d_model)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.w_2(self.dropout(self.relu(self.w_1(x))))"
      ],
      "metadata": {
        "id": "YPxf7PR9TOzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Section 3.4**:\n",
        "\n",
        "###Embeddings and Positional Encoding\n"
      ],
      "metadata": {
        "id": "we6qEjEsecyC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Embeddings\n",
        "\n",
        "We need to embed the input and output tokens into $d_{\\text{model}}$-dimensional vectors. We use learned embeddings to do this. We share the weight matrix between the pre-softmax linear transform in both input and output embedding layers."
      ],
      "metadata": {
        "id": "33ctoC6Guuex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Embeddings(nn.Module):\n",
        "  def __init__(self, d_model, vocab):\n",
        "    super(Embeddings, self).__init__()\n",
        "    self.lut = nn.Embedding(vocab, d_model)\n",
        "    self.d_model = d_model\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.lut(x)*math.sqrt(self.d_model)"
      ],
      "metadata": {
        "id": "d0M-i_2hf7dz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`self.lut` above uses a linear transform to use a lookup table embedding. We multiply the pre-softmax linear transformation output with $\\sqrt{d_{\\text{model}}}$."
      ],
      "metadata": {
        "id": "fHxFIfM1hN8m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Section 3.4**:\n",
        "\n",
        "####Positional Encoding\n",
        "We need a way to encode the relative or absolute positions of the words (tokens) in the sequence. We don't use any recurrence or convolution so we need to add information about token position in another way.\n",
        "\n",
        "We use sinusoids scaled based on the dimension and position to differentiate positional data in the embedding vectors.\n",
        "\n",
        "The exact details will become clear in the implementation below."
      ],
      "metadata": {
        "id": "ptP91-MXiVoo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self, d_model, dropout, max_len=5000):\n",
        "    super(PositionalEncoding, self).__init__()\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    #max_len is max length of sequences allowed.\n",
        "    pe = torch.zeros(max_len, d_model)\n",
        "    position = torch.arange(0, max_len).unsqueeze(1)\n",
        "    div_term = torch.exp(\n",
        "        torch.arange(0, d_model, 2)*-(math.log(10000.0)/d_model)\n",
        "    )\n",
        "    pe[:, 0::2] = torch.sin(position * div_term)\n",
        "    pe[:, 1::2] = torch.cos(position * div_term)\n",
        "    pe = pe.unsqueeze(0)\n",
        "\n",
        "    #register_buffer is used because it doesn't\n",
        "    #create a parameter. This means optimizer will not alter it\n",
        "    self.register_buffer(\"pe\", pe)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + self.pe[:, : x.size(1)].requires_grad_(False)\n",
        "    return self.dropout(x)"
      ],
      "metadata": {
        "id": "FRYYvHTPhKgX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#to get a better idea of how the positional encoding\n",
        "#sinewaves look, we can do some visualization\n",
        "def example_positional():\n",
        "    pe = PositionalEncoding(20, 0)\n",
        "    y = pe.forward(torch.zeros(1, 100, 20))\n",
        "\n",
        "    data = pd.concat(\n",
        "        [\n",
        "            pd.DataFrame(\n",
        "                {\n",
        "                    \"embedding\": y[0, :, dim],\n",
        "                    \"dimension\": dim,\n",
        "                    \"position\": list(range(100)),\n",
        "                }\n",
        "            )\n",
        "            for dim in [4, 5, 6, 7]\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return (\n",
        "        alt.Chart(data)\n",
        "        .mark_line()\n",
        "        .properties(width=800)\n",
        "        .encode(x=\"position\", y=\"embedding\", color=\"dimension:N\")\n",
        "        .interactive()\n",
        "    )\n",
        "\n",
        "\n",
        "show_example(example_positional)"
      ],
      "metadata": {
        "id": "pmZ7tFK_psWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###The TRANSFORMER\n"
      ],
      "metadata": {
        "id": "_hsOfZVRvwPq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now define a function that puts everything together to make a transformer model. All of the defaults for hyperparameters are identical to the hyperparameters used in the **Attention is All You Need** paper and have been mentioned in my descriptions earlier in this notebook."
      ],
      "metadata": {
        "id": "THZ1b20KxFbZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_transformer(\n",
        "    src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1\n",
        "    ):\n",
        "  c = copy.deepcopy\n",
        "  attn = MultiHeadAttention(h, d_model)\n",
        "  ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
        "  position = PositionalEncoding(d_model, dropout)\n",
        "  transformer = EncoderDecoder(\n",
        "      Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
        "      Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N),\n",
        "      nn.Sequential(Embeddings(d_model, src_vocab, c(position))),\n",
        "      nn.Sequential(Embeddings(d_model, tgt_vocab, c(position))),\n",
        "      Generator(d_model, tgt_vocab),\n",
        "  )\n",
        "  # This is taken from The Annotated Transformer\n",
        "  # Not sure why parameters are initialized in this specific manner\n",
        "  for p in transformer.parameters():\n",
        "      if p.dim() > 1:\n",
        "          nn.init.xavier_uniform_(p)\n",
        "  return transformer\n",
        ""
      ],
      "metadata": {
        "id": "B3HRQe2jwz-H"
      },
      "execution_count": 1,
      "outputs": []
    }
  ]
}