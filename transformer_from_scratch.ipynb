{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPavich/95wNBqfYJYLws5E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaroorhmodi/transformer-from-scratch/blob/main/transformer_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Building a Transformer from Scratch\n",
        "\n",
        "Using the famous paper [***Attention Is All You Need***](https://arxiv.org/pdf/1706.03762.pdf), we will build a transformer model from scratch and train it on the ~**tiny_shakespeare** dataset featured in [Andrej Karpathy's article](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)~. The new dataset is yet to be determined. Thinking about training it on the [Stanford Question Answering Dataset (SQuAD)](https://paperswithcode.com/dataset/squad). I am not expecting performance nearly as good as transfer learning with T5 but I would like to see something resembling good QA reasoning.\n",
        "\n",
        "Following [*The Annotated Transformer*](http://nlp.seas.harvard.edu/annotated-transformer/#training-data-and-batching) along with this [Towards Data Science post](https://towardsdatascience.com/all-you-need-to-know-about-attention-and-transformers-in-depth-understanding-part-1-552f0b41d021#8607) loosely to clarify any confusions when reading the paper.\n",
        "\n",
        "I will **NOT** be training the model on the same amount of data as the original paper does. It's about 3.5 days of training on 8 GPUs, something I cannot do at the moment. Perhaps I will attempt bigger models in the future! :)"
      ],
      "metadata": {
        "id": "KZktnWMrup34"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Preliminary Work"
      ],
      "metadata": {
        "id": "aX4xdt9_yk-b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torchdata==0.3.0 torchtext==0.12 spacy==3.2 altair GPUtil"
      ],
      "metadata": {
        "id": "zAC5uWTxuQ2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4LW_EzhFh-H"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import math\n",
        "import copy\n",
        "import time\n",
        "import pandas aspd\n",
        "import altair as alt\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "\n",
        "RUN_EXAMPLES = True\n",
        "\n",
        "#I will only ever run this on GPU\n",
        "#but I want to make this device agnostic.\n",
        "\n",
        "#FUTURE GOALS: Try to do multi-gpu training in colab in the\n",
        "#future with a newer, more ambitious paper to duplicate\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###PAPER COMMENTS:\n",
        "**Section 1** of the paper basically breaks down part of the motivation behind including attention mechanisms in NLP models. I am familiar with how LSTM and other RNN architectures work and it seems like the impetus (at the time) was to capture inter-word dependencies in a way that is not inherently sequential. This allows for training efficiency to improve by leveraging parallel computation (*which I am funnily enough eschewing in this exercise I am undertaking*). This parallelization effectively increases the effectiveness of the model by allowing for training on larger sets of data.\n",
        "\n",
        "**Section 2** of the paper breaks down how attention compares to existing approaches employing convolution and sequence-aligned RNNS ([which I think refers to Seq2Seq models](https://towardsdatascience.com/how-to-implement-seq2seq-lstm-model-in-keras-shortcutnlp-6f355f3e5639)).\n",
        "\n",
        "Attention has an advantage in learning relations between distant word positions (constant number of operations) but it loses some resolution. The Transformer employs **Multi-Head-Attention** to counteract this. This is described later in the paper."
      ],
      "metadata": {
        "id": "rNayhR5Pypt2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model Architecture"
      ],
      "metadata": {
        "id": "Pfz-7XCcMleJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most of the model architecture is covered in **Section 3**.\n",
        "\n",
        "The transformer model is similar to prior popular models like *LSTM* in that it has an **encoder-decoder** structure. Encoders map input sequences (symbol representation) to continuous representations which the decoder then maps to symbols. this happens one element at a time and is auto-regressive; which is to say that all previously generated symbols become inputs as the next output symbol is generated.\n",
        "\n",
        "For $\\mathbf{x} = (x_1,…, x_n)\\in \\mathbf{W}$, $\\mathbf{y} = (y_1,…, y_m)\\in \\mathbf{W'}$, $\\mathbf{z} = (z_1,…, z_n)\\in \\mathbb{R}^n$, the mapping looks something like this:\n",
        "$$\\mathbf{x}→\\mathbf{z}→\\mathbf{y}$$\n",
        "Above we have $\\mathbf{W}$ and $\\mathbf{W'}$ to represent word vocabularies which may or may not be in the same language (different languages might be used in a machine translation task like the original transformer paper used).  "
      ],
      "metadata": {
        "id": "4KRhpO41Mqqd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Encoder-Decoder Architecture](https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1-768x1082.png)"
      ],
      "metadata": {
        "id": "-d_KySRMIPdf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderDecoder(nn.Module):\n",
        "  \"\"\"\n",
        "  This is a skeleton for the encoder-decoder structuer mentioned above.\n",
        "\n",
        "  I am following the code organization in the Annotated Transformer article mentioned above.\n",
        "  \"\"\"\n",
        "  def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
        "    super(EncoderDecoder, self).__init__()\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.src_embed = src_embed\n",
        "    self.tgt_embed = tgt_embed\n",
        "    self.generator = generator\n",
        "\n",
        "  def encode(self, src, src_mask):\n",
        "    \"\"\"\n",
        "    Forward pass of the encoder. Embed the src text\n",
        "    and mask it. Then you run a fwd pass of the encoder on it.\n",
        "\n",
        "    This is the shell of the first half of the transformer's\n",
        "    forward pass. We will code the specfics of this when we\n",
        "    design the encoder.\n",
        "    \"\"\"\n",
        "    return self.encoder(self.src_embed(src), src_mask)\n",
        "\n",
        "  def decode(self, memory, src_mask, tgt, tgt_mask):\n",
        "    \"\"\"\n",
        "    This is the shell of the second half of the forward pass of\n",
        "    the transformer. We will also specify this further ahead.\n",
        "\n",
        "    The decoder has a memory component because it also takes in\n",
        "    the already generated words in its output.\n",
        "    \"\"\"\n",
        "    return self.decoder(self.tgt_embed(tgt), memory, tgt_mask)"
      ],
      "metadata": {
        "id": "yOovacTuOOaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***The Annotated Transformer*** also defines the generator portion of the transformer here. This is given in the ***Attention is All You Need*** paper by the linear portion following the decoder along with the softmax step following it. This linear layer is not the same as the linear layers that are in both the encoder and decoder."
      ],
      "metadata": {
        "id": "Skj7W9ib8MrZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "  \"\"\"\n",
        "  As described above, the Generator follows the Decoder.\n",
        "  This portion seems relatively simple.\n",
        "  It's simply a fully connected Linear step followed by softmax.\n",
        "  \"\"\"\n",
        "  def __init__(self, d_model, vocab):\n",
        "    super(Generator, self).__init__()\n",
        "    self.proj = nn.Linear(d_model, vocab)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return nn.log_softmax(self.proj(x), dim=1)"
      ],
      "metadata": {
        "id": "1qwbXpvz8MYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Section 3.1**: The paper describes the nature of the Encoder and Decoder here.\n",
        "\n",
        "###Encoder\n",
        "    \n",
        "The encoder has $N$ stacked layers that are all the same. Each layer has a Multi-Head Attention sublayer that feeds into a position-wise fully connected feed-forward network.The output of each sublayer is given by $\\text{LayerNorm}(x + \\text{Sublayer}(x))$ where $\\text{Sublayer}(x)$ is the forward pass function of the sublayer in question.\n",
        "\n",
        "`LayerNorm` is layer normalization. **The Annotated Transformer** implements it manually so I will do the same to avoid issues further ahead but I believe what is being done here is very similar to the implementation in [`torch.nn.LayerNorm`](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html).\n",
        "\n",
        "There is also the question of a residual connection between the sublayers alongside the norm. In the paper it is mentioned that a sublayer dropout is implemented before the normalization with $P_{\\text{drop}}=0.1$ in **Section 5.4**.\n",
        "\n",
        "In the paper the encoder has $N=6$ stacked layers."
      ],
      "metadata": {
        "id": "8cQ37aw4HSor"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "  def __init__(self, features, eps=1e-6):\n",
        "    super(LayerNorm, self).__init__()\n",
        "    self.a_2 = nn.Parameter(torch.ones(features))\n",
        "    self.b_2 = nn.Parameter(torch.zeros(features))\n",
        "    self.eps = eps\n",
        "\n",
        "  def forward(self, x):\n",
        "    mean = x.mean(-1, keepdim=True)\n",
        "    std = x.std(-1, keepdim=True)\n",
        "    return self.a_2*(x-mean)/(std+self.eps) +self.b_2"
      ],
      "metadata": {
        "id": "NSlYM-YHN2MQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SublayerConnection(nn.Module):\n",
        "  \"\"\"\n",
        "  Residual Connection with dropout followed by LayerNorm.\n",
        "\n",
        "  This accomplishes the aforementioned residual connection\n",
        "  with normalization used between the sublayers of the Encoder.\n",
        "\n",
        "  dropout has default prob=0.1 like in the paper.\n",
        "  \"\"\"\n",
        "  def __init__(self, size, dropout=0.1):\n",
        "    super(SublayerConnection, self).__init__()\n",
        "    self.norm = LayerNorm(size)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x, sublayer):\n",
        "    return x+self.dropout(sublayer(self.norm(x)))\n"
      ],
      "metadata": {
        "id": "__2cPxY7Q_bf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I think I see why this is self-implemented. This will do LayerNorm for multiple stacked layers as a single layer in an `EncoderLayer` implementation."
      ],
      "metadata": {
        "id": "do---aJfPk3e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clones(module, N):\n",
        "  #produces N-stack of identical layers\n",
        "  return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self, layer, N):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.layers = clones(layer, N)\n",
        "    self.norm = LayerNorm(layer.size)\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "    \"Forward pass of the encoder. Include mask.\"\n",
        "    for layer in self.layers:\n",
        "      x = layer(x, mask)\n",
        "    return self.norm(x)"
      ],
      "metadata": {
        "id": "c9Boxl8KyoYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "  \"\"\"\n",
        "  This is the fundamental architecture of a single Encoder Layer.\n",
        "\n",
        "  The encoder will be made up of N stacked copies of this (N=6).\n",
        "\n",
        "  We have yet to implement the key feature: Multi-Head Attention.\n",
        "\n",
        "  This will be implemented and explained below.\n",
        "  \"\"\"\n",
        "  def __init__(self, size, self_attn, feed_forward, dropout=0.1):\n",
        "    super(EncoderLayer, self).__init__()\n",
        "    self.self_attn = self_attn #multi-head attention layer\n",
        "    self.feed_forward = feed_forward\n",
        "    # N=2 sublayer connections here are not to be thought of\n",
        "    # as \"stacked\" but rather they are just the sublayer connection\n",
        "    # following the MHA layer and then the feedforward layer.\n",
        "    self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
        "    self.size = size\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "    x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
        "    return self.sublayer[1](x, self.feed_forward)\n"
      ],
      "metadata": {
        "id": "8WWmtpG2Tghz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Decoder**\n",
        "    \n",
        "The decoder is similar to the encoder in that it employs $N=6$ stacked layers that are exactly alike. It also shares a similar sublayer connection to the Encoder but it has *two* MHA sublayers.\n",
        "\n",
        "The first performs masked MHA over the output embedding (the mask is there to prevent signal from later words in the output from reaching prediction on earlier words).\n",
        "\n",
        "The second sublayer performs MHA over the output from the encoder and maintains the sublayer scheme from before.\n",
        "\n",
        "This all goes into the third sublayer (feed-forward) in a similar fashion to the encoder layer."
      ],
      "metadata": {
        "id": "i7eRs5HQMmq8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "\n",
        "  def __init__(self, layer, N):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.layers = clones(layer, N)\n",
        "    self.norm = LayerNorm(layer.size)\n",
        "\n",
        "  def forward(self, x, memory, src_mask, tgt_mask):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x, memory, src_mask, tgt_mask)\n",
        "      return self.norm(x)"
      ],
      "metadata": {
        "id": "Gkq0uBseMble"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will make the `DecoderLayer` class using the `SublayerConnection` class we made before. MHA is yet to be implemented, it will be accounted for in the `DecoderLayer` class."
      ],
      "metadata": {
        "id": "A9aF4pNVpRtr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "\n",
        "  def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "    self.size = size\n",
        "    self.self_attn = self_attn\n",
        "    self.src_attn = src_attn\n",
        "    self.feed_forward = feed_forward\n",
        "    self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
        "\n",
        "  def forward(self, x, memory, src_mask, tgt_mask):\n",
        "    m = memory\n",
        "    x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
        "    x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n",
        "    return self.sublayer[2](x, self.feed_forward)"
      ],
      "metadata": {
        "id": "sVlN6-qOpHcQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to include the mask to mask out subsequent words in the output. This way we avoid the signal from later words (or positions) in the output from reaching earlier ones."
      ],
      "metadata": {
        "id": "XQUk3jSEshHR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def subsequent_mask(size):\n",
        "  attn_shape = (1,shape,shape)\n",
        "  subsequent_mask = torch.triu(torch.ones(attn_shape), diagonal=1).type(torch.uint8)\n",
        "  return subsequent_mask == 0"
      ],
      "metadata": {
        "id": "aODuGPzDtLtN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Attention\n",
        "\n",
        "In the paper there are two parts to *Multi-Headed Attention*. There is **Scaled Dot-Product Attention** which feeds into **Multi-Headed Attention**."
      ],
      "metadata": {
        "id": "gFdP7Xovx58Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://machinelearningmastery.com/wp-content/uploads/2022/03/dotproduct_1.png)"
      ],
      "metadata": {
        "id": "5VrCjoXtz8p2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#First we implement Scaled Dot Product Attention\n",
        "def attention(Q:torch.Tensor, K:torch.Tensor, V:torch.Tensor, mask=None, dropout=None):\n",
        "  d_k = Q.size(-1)\n",
        "  scores = torch.matmul(Q, K.transpose(-2,-1)) / math.sqrt(d_k)\n",
        "  if mask:\n",
        "    #we fill zeroes with something like -inf\n",
        "    #this is to bring them close to 0 in softmax\n",
        "    scores = scores.masked_fill(mask == 0, -1e9)\n",
        "  if dropout:\n",
        "    p_attn = dropout(p_attn)\n",
        "  return torch.matmul(p_attn, V), p_attn\n"
      ],
      "metadata": {
        "id": "-EiIWFi0z8OC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4uwG9jWrv2-g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}